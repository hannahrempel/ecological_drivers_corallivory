---
title: "Data processing: Cleaning and merging fish survery data"
author: "Hannah Rempel"
date: "Last updated 14/March/2024"
output: html_document
---
Cleans fish data from Panama 2013, Florida 2013, St. Croix 2018, and Bonaire 2018 then merges data together for ecological drivers of corallivory analysis

#Setup 
```{r Setup}
if (!require(here)) install.packages("here")
if (!require(purrr)) install.packages("purrr")
if (!require(tidyverse)) install.packages("tidyverse")

packages <- c("here", "purrr", "tidyverse")
sapply(packages, require, character.only = T)
```

```{r Reading in data}
stx18_bon19 <- read_csv(here("data/stx18_bon19_fish_data.csv")) %>% 
  mutate(date=as.Date(date, "%m/%d/%y"))

fla13_pan13 <- read.csv(here("data/fla13_pan13_fish_data.csv"), fileEncoding = "UTF-8") %>% #so accents are read in correctly
  mutate(date=as.Date(date))

fish_ab_vals <- read_csv(here("data/fish_ab_values.csv"))
```
#Cleaning St. Croix and Bonaire data
Checking the average length of survey segments to decide which interval both is most comparable with other surveys (25x4m in length, 100m2) and still maintains a sufficient number of replicates per site. Based on this, the interval of 16x5 (80m2) to 20x5 (100m2) segments both are fairly comparable to other surveys and maximize the number of replicates
```{r Subsetting STX 18 and BON 19 fish surveys to 15-20m segments}
checking_average_segment_length <- stx18_bon19 %>% 
  mutate(dist_m_rnd =round(dist_m, 0)) %>%
  select(site, transect, min, dist_m_rnd) %>%
  distinct() %>% 
  filter(dist_m_rnd >=15 & dist_m_rnd <=25)
  
hist(checking_average_segment_length$dist_m_rnd)

#subsetting data to only include surveys between 16 to 20m
fish_df_16_to_20 <- stx18_bon19 %>%   
  filter(dist_m >=16 & dist_m <=20) %>%
  mutate(dist_m =round(dist_m, 0)) %>%
  select(region, date, site, transect, species, stage, fl_cm, n_pf, dist_m, min, depth_m)
```

The following chunk filters the data so that fish survey segments are not collected in back-to-back minutes, ensuring that there is at least 16m between every survey. Here is a description of the process:

(1) Filters the data to only include distinct region, date, site, transect, minute combinations
(2) Within the cumulative sum statement, the first logical statement returns TRUE (1) if the min for a given row is equal to the min +1 of the previous row, or FALSE (0) if not
The second logical statement within cumsum returns TRUE if the value is NA (which it is for the first row in the dataset only)
The cumsum component takes the cumulative sum of the TRUE (1) and FALSE (0) rowwise, so segments that are in non-consecutive minutes will have distinctive segment_id values, while those in consecutive minutes will not
(3) Splits the singular dataframe into many dataframes grouped by their segment ID and within each group assigns a row number (seq(1:n())), then creates a row_to_keep variable is TRUE for odd rows, and FALSE for even rows. The split dataframes are then bound together back into one dataframe
(4) The rows with 'FALSE' are filtered out, which keeps the first segment in any consecutive minute, removes the next, keeps the next, so on and so forth for 1:n() back to back minutes of surveys. This ensures that there is a gap of at least 16m between each survey segment.

In the second part of the chunk, the filter index is  
```{r Filtering every other consecutive minutes}
segment_filter_df <- fish_df_16_to_20 %>% 
  select(region, date, site, transect, min) %>% 
  distinct() %>%
  mutate(segment_id=cumsum(distinct_segment= min != lag(min) + 1 | is.na(min != lag(min) + 1))) %>%
  group_split(segment_id) %>% 
  map(~.x %>% mutate(row_num=row_number(), row_to_keep=row_num %%2==1)) %>%
  bind_rows() %>%
  select(-segment_id, -row_num)

fish_df_filtered <- fish_df_16_to_20 %>% 
  dplyr::left_join(segment_filter_df) %>%
  filter(row_to_keep==TRUE) 

fish_replicate_check <- fish_df_filtered %>%
  select(region, date, site, transect, min, dist_m) %>%
  distinct() %>%
  group_by(region, site) %>%
  dplyr::reframe(n=n())
```

Data were originally collected in 20-25 min swims with a GPS tow, with a 5m wide suvey swath. To integrate these data with Florida and Panama data (collected in 25x4m transects), we selected segments from the surveys that were between 15-20m length (so surveys are 15x5 to 20x5m). If any surveys were in back to back minutes, one of these was swapped so there was at least 15m between each survey segment. 

In the following chunk, these segments are now assigned a unique transect number by region and site for each of the 15-20m segments.
```{r Final clean of BON and STX data}
stx_bon19_transect_temp <- fish_df_filtered %>% 
  select(region, site, date, transect_old=transect, min, dist_m, depth_m) %>% 
  distinct() %>%
  group_by(region, site) %>%
  mutate(transect=1:n())

stx_bon19_metadata <- stx_bon19_transect_temp %>% 
  mutate(survey_w_m=5) %>%
  select(region, site, date, transect, depth_m, survey_l_m= dist_m, survey_w_m) %>%
  mutate(depth_m=round(depth_m, 1)) %>%
  distinct()

stx18_bon19_clean <- fish_df_filtered %>% 
  rename(transect_old=transect, number=n_pf, phase=stage, size_cm=fl_cm) %>%
  dplyr::left_join(stx_bon19_transect_temp, by = c("region", "date", "site", "min", "transect_old", "depth_m")) %>%
  select(region, site, date, transect, depth_m, species,phase, size_cm, number) %>%
  mutate(depth_m=round(depth_m, 1),
         size_cm=as.character(size_cm)) 
```

#Merging data from all regions
```{r merging data from all regions}
fish_survey_metadata <- fla13_pan13 %>% 
  mutate(survey_l_m=25, survey_w_m=4) %>%
  select(region, site, date, transect, depth_m, survey_l_m, survey_w_m) %>% 
  distinct() %>%
  bind_rows(stx_bon19_metadata)

merged_fish_surveys <- fla13_pan13 %>% 
  filter(number>0) %>% #filtering to only include observations with fish
  uncount(number) %>%
  bind_rows(stx18_bon19_clean) %>%
  select(-number)%>%
  mutate(count=1)

major_corallivores_df <-  merged_fish_surveys %>% 
  filter(species %in% c("Scarus taeniopterus", "Scarus vetula", "Sparisoma viride")) #filtering to only include predominant corallivore species

major_corallivore_counts_df <- fish_survey_metadata %>%
  dplyr::left_join(major_corallivores_df) %>%
  mutate(species=replace_na(species, "No major corallivores"), #accounting for transects with no observed major corallivores
         count=replace_na(count, 0))
```

Filtering length to weight conversions for the predominant corallivore species of interest to only include values with an R2 greater than 0.85, then from these data filtering to take the a-b values with the highest number of observations per species. Note that observations only include parrotfishes that were at least 15cm fork length and data are filtered to only include major corallivores. We classified major corallivores as species that, on average, take at least 1% of their foraging bites on coral: *Sc. taeniopterus, Sc. vetula, Sp. viride*. See manuscript for details. 

```{r filtering fish ab values}
filtered_fish_ab_vals <- fish_ab_vals %>% 
  filter(species %in% c("Scarus taeniopterus", "Scarus vetula", "Sparisoma viride")) %>% #selecting species of interest
  group_by(species) %>% 
  filter(rsquared>0.85 | is.na(rsquared)) %>% #filtering out values with low R2
  slice_max(n_obs, n=1) %>% #from values with R2>0.85, selecting those with the max species observed 
  select(species, a, b) 
```

```{r Estimating fish weight per individual}
major_corallivore_counts_weights_df <-major_corallivore_counts_df %>%
  left_join(filtered_fish_ab_vals) %>%
  mutate(size_cm_est=case_when( #NAs introduced by coersion warning caused by NA's for transects with no fish (i.e., not an issue)
    size_cm=="15 to 20"~17.5, #midpoint within size bin
    size_cm=="20 to 25"~22.5, 
    size_cm=="25 to 30"~27.5,
    size_cm=="30 to 35"~32.5,
    TRUE~as.numeric(size_cm))
    ) %>%  
  mutate(weight_g=a*size_cm_est^b) %>% #calculating estimated fish weight per individual
  mutate(weight_g=case_when(is.na(weight_g)~0, #adding in 0's for cases where no fish were observed in a transect 
                   TRUE~as.numeric(weight_g)))
```

```{r writing data to csv}
write.csv(major_corallivore_counts_weights_df, 
          here("data/regional_fish_data.csv"), 
          fileEncoding = "UTF-8",
          row.names = FALSE)
```